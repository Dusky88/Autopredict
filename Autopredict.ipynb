{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b61e4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import pickle\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8c5dc88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Project Gutenberg eBook of Middlemarch, by George Eliot This eBook is for the use of anyone anywhere in the United States and most other parts of the world at no cost and with almost no restrictions whatsoever. You may copy it, give it away or re-use it under the terms of the Project Gutenberg License included with this eBook or online at www.gutenberg.org. If you are not located in the United States, you will have to check the laws of the country where you are located before using this eBoo'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = open(\"Middlemarch.txt\", \"r\", encoding = \"utf8\")\n",
    "\n",
    "# store file in list\n",
    "lines = []\n",
    "for i in file:\n",
    "    lines.append(i)\n",
    "\n",
    "# Convert list to string\n",
    "data = \"\"\n",
    "for i in lines:\n",
    "  data = ' '. join(lines) \n",
    "\n",
    "#replace unnecessary stuff with space\n",
    "data = data.replace('\\n', '').replace('\\r', '').replace('\\ufeff', '').replace('“','').replace('”','')  #new line, carriage return, unicode character --> replace by space\n",
    "\n",
    "#remove unnecessary spaces \n",
    "data = data.split()\n",
    "data = ' '.join(data)\n",
    "data[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64a1ab31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56657"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ed360cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 517, 354, 274, 2, 886, 34, 887, 888, 37, 274, 20, 22, 1, 275]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([data])\n",
    "\n",
    "# saving the tokenizer for predict function\n",
    "pickle.dump(tokenizer, open('token.pkl', 'wb'))\n",
    "\n",
    "sequence_data = tokenizer.texts_to_sequences([data])[0]\n",
    "sequence_data[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91df7173",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10064"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sequence_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94cf2bc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2538\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a2a865b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Length of sequences are:  10061\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  1, 517, 354, 274],\n",
       "       [517, 354, 274,   2],\n",
       "       [354, 274,   2, 886],\n",
       "       [274,   2, 886,  34],\n",
       "       [  2, 886,  34, 887],\n",
       "       [886,  34, 887, 888],\n",
       "       [ 34, 887, 888,  37],\n",
       "       [887, 888,  37, 274],\n",
       "       [888,  37, 274,  20],\n",
       "       [ 37, 274,  20,  22]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences = []\n",
    "\n",
    "for i in range(3, len(sequence_data)):\n",
    "    words = sequence_data[i-3:i+1]\n",
    "    sequences.append(words)\n",
    "    \n",
    "print(\"The Length of sequences are: \", len(sequences))\n",
    "sequences = np.array(sequences)\n",
    "sequences[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dddd71dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "\n",
    "for i in sequences:\n",
    "    X.append(i[0:3])\n",
    "    y.append(i[3])\n",
    "    \n",
    "X = np.array(X)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "80afbb69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data:  [[  1 517 354]\n",
      " [517 354 274]\n",
      " [354 274   2]\n",
      " [274   2 886]\n",
      " [  2 886  34]\n",
      " [886  34 887]\n",
      " [ 34 887 888]\n",
      " [887 888  37]\n",
      " [888  37 274]\n",
      " [ 37 274  20]]\n",
      "Response:  [274   2 886  34 887 888  37 274  20  22]\n"
     ]
    }
   ],
   "source": [
    "print(\"Data: \", X[:10])\n",
    "print(\"Response: \", y[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc29ebea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2da25bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 10, input_length=3))\n",
    "model.add(LSTM(1000, return_sequences=True))\n",
    "model.add(LSTM(1000))\n",
    "model.add(Dense(1000, activation=\"relu\"))\n",
    "model.add(Dense(vocab_size, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "77377bb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 3, 10)             25380     \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 3, 1000)           4044000   \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 1000)              8004000   \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1000)              1001000   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2538)              2540538   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 15,614,918\n",
      "Trainable params: 15,614,918\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b06d0705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "158/158 [==============================] - ETA: 0s - loss: 6.8320\n",
      "Epoch 1: loss improved from inf to 6.83203, saving model to next_words.h5\n",
      "158/158 [==============================] - 73s 435ms/step - loss: 6.8320\n",
      "Epoch 2/50\n",
      "158/158 [==============================] - ETA: 0s - loss: 6.4218\n",
      "Epoch 2: loss improved from 6.83203 to 6.42176, saving model to next_words.h5\n",
      "158/158 [==============================] - 67s 428ms/step - loss: 6.4218\n",
      "Epoch 3/50\n",
      "158/158 [==============================] - ETA: 0s - loss: 6.3244\n",
      "Epoch 3: loss improved from 6.42176 to 6.32442, saving model to next_words.h5\n",
      "158/158 [==============================] - 58s 366ms/step - loss: 6.3244\n",
      "Epoch 4/50\n",
      "158/158 [==============================] - ETA: 0s - loss: 6.1194\n",
      "Epoch 4: loss improved from 6.32442 to 6.11941, saving model to next_words.h5\n",
      "158/158 [==============================] - 59s 374ms/step - loss: 6.1194\n",
      "Epoch 5/50\n",
      "158/158 [==============================] - ETA: 0s - loss: 5.8467\n",
      "Epoch 5: loss improved from 6.11941 to 5.84666, saving model to next_words.h5\n",
      "158/158 [==============================] - 60s 378ms/step - loss: 5.8467\n",
      "Epoch 6/50\n",
      "158/158 [==============================] - ETA: 0s - loss: 5.5980\n",
      "Epoch 6: loss improved from 5.84666 to 5.59803, saving model to next_words.h5\n",
      "158/158 [==============================] - 63s 398ms/step - loss: 5.5980\n",
      "Epoch 7/50\n",
      "158/158 [==============================] - ETA: 0s - loss: 5.3771\n",
      "Epoch 7: loss improved from 5.59803 to 5.37705, saving model to next_words.h5\n",
      "158/158 [==============================] - 62s 392ms/step - loss: 5.3771\n",
      "Epoch 8/50\n",
      "158/158 [==============================] - ETA: 0s - loss: 5.2011\n",
      "Epoch 8: loss improved from 5.37705 to 5.20111, saving model to next_words.h5\n",
      "158/158 [==============================] - 67s 426ms/step - loss: 5.2011\n",
      "Epoch 9/50\n",
      "158/158 [==============================] - ETA: 0s - loss: 5.0253\n",
      "Epoch 9: loss improved from 5.20111 to 5.02535, saving model to next_words.h5\n",
      "158/158 [==============================] - 71s 450ms/step - loss: 5.0253\n",
      "Epoch 10/50\n",
      "158/158 [==============================] - ETA: 0s - loss: 4.8587\n",
      "Epoch 10: loss improved from 5.02535 to 4.85873, saving model to next_words.h5\n",
      "158/158 [==============================] - 63s 400ms/step - loss: 4.8587\n",
      "Epoch 11/50\n",
      "158/158 [==============================] - ETA: 0s - loss: 4.6837\n",
      "Epoch 11: loss improved from 4.85873 to 4.68371, saving model to next_words.h5\n",
      "158/158 [==============================] - 65s 413ms/step - loss: 4.6837\n",
      "Epoch 12/50\n",
      "158/158 [==============================] - ETA: 0s - loss: 4.4812\n",
      "Epoch 12: loss improved from 4.68371 to 4.48123, saving model to next_words.h5\n",
      "158/158 [==============================] - 61s 387ms/step - loss: 4.4812\n",
      "Epoch 13/50\n",
      "158/158 [==============================] - ETA: 0s - loss: 4.2609\n",
      "Epoch 13: loss improved from 4.48123 to 4.26093, saving model to next_words.h5\n",
      "158/158 [==============================] - 62s 396ms/step - loss: 4.2609\n",
      "Epoch 14/50\n",
      "158/158 [==============================] - ETA: 0s - loss: 4.0156\n",
      "Epoch 14: loss improved from 4.26093 to 4.01562, saving model to next_words.h5\n",
      "158/158 [==============================] - 61s 384ms/step - loss: 4.0156\n",
      "Epoch 15/50\n",
      "158/158 [==============================] - ETA: 0s - loss: 3.7663\n",
      "Epoch 15: loss improved from 4.01562 to 3.76633, saving model to next_words.h5\n",
      "158/158 [==============================] - 61s 387ms/step - loss: 3.7663\n",
      "Epoch 16/50\n",
      "158/158 [==============================] - ETA: 0s - loss: 3.5080\n",
      "Epoch 16: loss improved from 3.76633 to 3.50800, saving model to next_words.h5\n",
      "158/158 [==============================] - 64s 404ms/step - loss: 3.5080\n",
      "Epoch 17/50\n",
      "158/158 [==============================] - ETA: 0s - loss: 3.2365\n",
      "Epoch 17: loss improved from 3.50800 to 3.23645, saving model to next_words.h5\n",
      "158/158 [==============================] - 75s 476ms/step - loss: 3.2365\n",
      "Epoch 18/50\n",
      "158/158 [==============================] - ETA: 0s - loss: 2.9865\n",
      "Epoch 18: loss improved from 3.23645 to 2.98645, saving model to next_words.h5\n",
      "158/158 [==============================] - 81s 510ms/step - loss: 2.9865\n",
      "Epoch 19/50\n",
      "158/158 [==============================] - ETA: 0s - loss: 2.7152\n",
      "Epoch 19: loss improved from 2.98645 to 2.71523, saving model to next_words.h5\n",
      "158/158 [==============================] - 62s 396ms/step - loss: 2.7152\n",
      "Epoch 20/50\n",
      "158/158 [==============================] - ETA: 0s - loss: 2.4689\n",
      "Epoch 20: loss improved from 2.71523 to 2.46892, saving model to next_words.h5\n",
      "158/158 [==============================] - 62s 392ms/step - loss: 2.4689\n",
      "Epoch 21/50\n",
      "158/158 [==============================] - ETA: 0s - loss: 2.2394\n",
      "Epoch 21: loss improved from 2.46892 to 2.23939, saving model to next_words.h5\n",
      "158/158 [==============================] - 61s 388ms/step - loss: 2.2394\n",
      "Epoch 22/50\n",
      "158/158 [==============================] - ETA: 0s - loss: 2.0201\n",
      "Epoch 22: loss improved from 2.23939 to 2.02009, saving model to next_words.h5\n",
      "158/158 [==============================] - 67s 424ms/step - loss: 2.0201\n",
      "Epoch 23/50\n",
      "158/158 [==============================] - ETA: 0s - loss: 1.7954\n",
      "Epoch 23: loss improved from 2.02009 to 1.79541, saving model to next_words.h5\n",
      "158/158 [==============================] - 68s 430ms/step - loss: 1.7954\n",
      "Epoch 24/50\n",
      "158/158 [==============================] - ETA: 0s - loss: 1.6113\n",
      "Epoch 24: loss improved from 1.79541 to 1.61125, saving model to next_words.h5\n",
      "158/158 [==============================] - 68s 432ms/step - loss: 1.6113\n",
      "Epoch 25/50\n",
      "158/158 [==============================] - ETA: 0s - loss: 1.4283\n",
      "Epoch 25: loss improved from 1.61125 to 1.42826, saving model to next_words.h5\n",
      "158/158 [==============================] - 67s 425ms/step - loss: 1.4283\n",
      "Epoch 26/50\n",
      "158/158 [==============================] - ETA: 0s - loss: 1.2603\n",
      "Epoch 26: loss improved from 1.42826 to 1.26035, saving model to next_words.h5\n",
      "158/158 [==============================] - 68s 430ms/step - loss: 1.2603\n",
      "Epoch 27/50\n",
      "158/158 [==============================] - ETA: 0s - loss: 1.1307\n",
      "Epoch 27: loss improved from 1.26035 to 1.13067, saving model to next_words.h5\n",
      "158/158 [==============================] - 67s 426ms/step - loss: 1.1307\n",
      "Epoch 28/50\n",
      "158/158 [==============================] - ETA: 0s - loss: 0.9994\n",
      "Epoch 28: loss improved from 1.13067 to 0.99937, saving model to next_words.h5\n",
      "158/158 [==============================] - 68s 428ms/step - loss: 0.9994\n",
      "Epoch 29/50\n",
      "158/158 [==============================] - ETA: 0s - loss: 0.8591\n",
      "Epoch 29: loss improved from 0.99937 to 0.85908, saving model to next_words.h5\n",
      "158/158 [==============================] - 68s 431ms/step - loss: 0.8591\n",
      "Epoch 30/50\n",
      "158/158 [==============================] - ETA: 0s - loss: 0.7509\n",
      "Epoch 30: loss improved from 0.85908 to 0.75091, saving model to next_words.h5\n",
      "158/158 [==============================] - 67s 425ms/step - loss: 0.7509\n",
      "Epoch 31/50\n",
      "158/158 [==============================] - ETA: 0s - loss: 0.6589\n",
      "Epoch 31: loss improved from 0.75091 to 0.65894, saving model to next_words.h5\n",
      "158/158 [==============================] - 69s 434ms/step - loss: 0.6589\n",
      "Epoch 32/50\n",
      "158/158 [==============================] - ETA: 0s - loss: 0.5736\n",
      "Epoch 32: loss improved from 0.65894 to 0.57362, saving model to next_words.h5\n",
      "158/158 [==============================] - 68s 429ms/step - loss: 0.5736\n",
      "Epoch 33/50\n",
      "158/158 [==============================] - ETA: 0s - loss: 0.5361\n",
      "Epoch 33: loss improved from 0.57362 to 0.53607, saving model to next_words.h5\n",
      "158/158 [==============================] - 68s 431ms/step - loss: 0.5361\n",
      "Epoch 34/50\n",
      "158/158 [==============================] - ETA: 0s - loss: 0.4356\n",
      "Epoch 34: loss improved from 0.53607 to 0.43556, saving model to next_words.h5\n",
      "158/158 [==============================] - 65s 410ms/step - loss: 0.4356\n",
      "Epoch 35/50\n",
      "158/158 [==============================] - ETA: 0s - loss: 0.3765\n",
      "Epoch 35: loss improved from 0.43556 to 0.37651, saving model to next_words.h5\n",
      "158/158 [==============================] - 65s 411ms/step - loss: 0.3765\n",
      "Epoch 36/50\n",
      "158/158 [==============================] - ETA: 0s - loss: 0.3615\n",
      "Epoch 36: loss improved from 0.37651 to 0.36153, saving model to next_words.h5\n",
      "158/158 [==============================] - 69s 435ms/step - loss: 0.3615\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/50\n",
      "158/158 [==============================] - ETA: 0s - loss: 0.3284\n",
      "Epoch 37: loss improved from 0.36153 to 0.32843, saving model to next_words.h5\n",
      "158/158 [==============================] - 66s 418ms/step - loss: 0.3284\n",
      "Epoch 38/50\n",
      "158/158 [==============================] - ETA: 0s - loss: 0.3280\n",
      "Epoch 38: loss improved from 0.32843 to 0.32800, saving model to next_words.h5\n",
      "158/158 [==============================] - 66s 415ms/step - loss: 0.3280\n",
      "Epoch 39/50\n",
      "158/158 [==============================] - ETA: 0s - loss: 0.2911\n",
      "Epoch 39: loss improved from 0.32800 to 0.29106, saving model to next_words.h5\n",
      "158/158 [==============================] - 65s 412ms/step - loss: 0.2911\n",
      "Epoch 40/50\n",
      "158/158 [==============================] - ETA: 0s - loss: 0.2847\n",
      "Epoch 40: loss improved from 0.29106 to 0.28469, saving model to next_words.h5\n",
      "158/158 [==============================] - 64s 404ms/step - loss: 0.2847\n",
      "Epoch 41/50\n",
      "158/158 [==============================] - ETA: 0s - loss: 0.2679\n",
      "Epoch 41: loss improved from 0.28469 to 0.26791, saving model to next_words.h5\n",
      "158/158 [==============================] - 64s 404ms/step - loss: 0.2679\n",
      "Epoch 42/50\n",
      "158/158 [==============================] - ETA: 0s - loss: 0.2419\n",
      "Epoch 42: loss improved from 0.26791 to 0.24185, saving model to next_words.h5\n",
      "158/158 [==============================] - 64s 407ms/step - loss: 0.2419\n",
      "Epoch 43/50\n",
      "158/158 [==============================] - ETA: 0s - loss: 0.2138\n",
      "Epoch 43: loss improved from 0.24185 to 0.21378, saving model to next_words.h5\n",
      "158/158 [==============================] - 66s 418ms/step - loss: 0.2138\n",
      "Epoch 44/50\n",
      "158/158 [==============================] - ETA: 0s - loss: 0.1936\n",
      "Epoch 44: loss improved from 0.21378 to 0.19360, saving model to next_words.h5\n",
      "158/158 [==============================] - 62s 392ms/step - loss: 0.1936\n",
      "Epoch 45/50\n",
      "158/158 [==============================] - ETA: 0s - loss: 0.2054\n",
      "Epoch 45: loss did not improve from 0.19360\n",
      "158/158 [==============================] - 66s 419ms/step - loss: 0.2054\n",
      "Epoch 46/50\n",
      "158/158 [==============================] - ETA: 0s - loss: 0.2111\n",
      "Epoch 46: loss did not improve from 0.19360\n",
      "158/158 [==============================] - 57s 359ms/step - loss: 0.2111\n",
      "Epoch 47/50\n",
      "158/158 [==============================] - ETA: 0s - loss: 0.2424\n",
      "Epoch 47: loss did not improve from 0.19360\n",
      "158/158 [==============================] - 55s 346ms/step - loss: 0.2424\n",
      "Epoch 48/50\n",
      "158/158 [==============================] - ETA: 0s - loss: 0.2283\n",
      "Epoch 48: loss did not improve from 0.19360\n",
      "158/158 [==============================] - 54s 343ms/step - loss: 0.2283\n",
      "Epoch 49/50\n",
      "158/158 [==============================] - ETA: 0s - loss: 0.2159\n",
      "Epoch 49: loss did not improve from 0.19360\n",
      "158/158 [==============================] - 55s 348ms/step - loss: 0.2159\n",
      "Epoch 50/50\n",
      "158/158 [==============================] - ETA: 0s - loss: 0.2014\n",
      "Epoch 50: loss did not improve from 0.19360\n",
      "158/158 [==============================] - 57s 360ms/step - loss: 0.2014\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x27a653d6eb0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "checkpoint = ModelCheckpoint(\"next_words.h5\", monitor='loss', verbose=1, save_best_only=True)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=Adam(learning_rate=0.001))\n",
    "model.fit(X, y, epochs=50, batch_size=64, callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "412473a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model = load_model('next_words.h5')\n",
    "tokenizer = pickle.load(open('token.pkl', 'rb'))\n",
    "\n",
    "def Predict_Next_Words(model, tokenizer, text):\n",
    "\n",
    "  sequence = tokenizer.texts_to_sequences([text])\n",
    "  sequence = np.array(sequence)\n",
    "  preds = np.argmax(model.predict(sequence))\n",
    "  predicted_word = \"\"\n",
    "  \n",
    "  for key, value in tokenizer.word_index.items():\n",
    "      if value == preds:\n",
    "          predicted_word = key\n",
    "          break\n",
    "  \n",
    "  print(predicted_word)\n",
    "  return predicted_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "76c36aee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your line: the life of\n",
      "['the', 'life', 'of']\n",
      "1/1 [==============================] - 1s 933ms/step\n",
      "saint\n",
      "Enter your line: the life of Saint\n",
      "['life', 'of', 'Saint']\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "theresa\n",
      "Enter your line: 0\n",
      "Execution completed.....\n"
     ]
    }
   ],
   "source": [
    "while(True):\n",
    "  text = input(\"Enter your line: \")\n",
    "  \n",
    "  if text == \"0\":\n",
    "      print(\"Execution completed.....\")\n",
    "      break\n",
    "  \n",
    "  else:\n",
    "      try:\n",
    "          text = text.split(\" \")\n",
    "          text = text[-3:]\n",
    "          print(text)\n",
    "        \n",
    "          Predict_Next_Words(model, tokenizer, text)\n",
    "          \n",
    "      except Exception as e:\n",
    "        print(\"Error occurred: \",e)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e23f463",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
